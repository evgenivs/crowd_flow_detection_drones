{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18UKdpclatR5"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras.layers import Conv2D, MaxPool2D, Input, Conv2DTranspose, BatchNormalization, Rescaling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input layer\n",
        "input_layer = Input(shape=(512,640,3), name='input_layer')\n",
        "x = Rescaling(scale=1/255.0)(input_layer)\n",
        "\n",
        "# Hidden layers - two branches\n",
        "# Encoder\n",
        "x_centroids_1 = Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_uniform')(x)\n",
        "x_centroids_1 = BatchNormalization()(x_centroids_1)\n",
        "x_centroids_1 = MaxPooling2D(pool_size=(2,2))(x_centroids_1)\n",
        "x_heads_1 = Conv2D(filters=16, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_uniform')(x)\n",
        "x_heads_1 = BatchNormalization()(x_heads_1)\n",
        "x_heads_1_pool = MaxPooling2D(pool_size=(2,2))(x_heads_1)\n",
        "\n",
        "x_centroids_2 = Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_uniform')(x_centroids_1)\n",
        "x_centroids_2 = BatchNormalization()(x_centroids_2)\n",
        "x_centroids_2 = MaxPooling2D(pool_size=(2,2))(x_centroids_2)\n",
        "x_heads_2 = Conv2D(filters=32, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_uniform')(x_heads_1_pool)\n",
        "x_heads_2 = BatchNormalization()(x_heads_2)\n",
        "x_heads_2_pool = MaxPooling2D(pool_size=(2,2))(x_heads_2)\n",
        "\n",
        "x_centroids_3 = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_uniform')(x_centroids_2)\n",
        "x_centroids_3 = BatchNormalization()(x_centroids_3)\n",
        "x_centroids_3 = MaxPooling2D(pool_size=(2,2))(x_centroids_3)\n",
        "x_heads_3 = Conv2D(filters=64, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_uniform')(x_heads_2_pool)\n",
        "x_heads_3 = BatchNormalization()(x_heads_3)\n",
        "x_heads_3_pool = MaxPooling2D(pool_size=(2,2))(x_heads_3)\n",
        "\n",
        "x_centroids_4 = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_uniform')(x_centroids_3)\n",
        "x_centroids_4 = BatchNormalization()(x_centroids_4)\n",
        "x_centroids_4 = MaxPooling2D(pool_size=(2,2))(x_centroids_4)\n",
        "x_centroids_4 = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_uniform')(x_centroids_4)\n",
        "x_centroids_4 = BatchNormalization()(x_centroids_4)\n",
        "x_heads_4 = Conv2D(filters=128,kernel_size=3, padding='same', activation='relu', kernel_initializer='he_uniform')(x_heads_3_pool)\n",
        "x_heads_4 = BatchNormalization()(x_heads_4)\n",
        "x_heads_4_pool = MaxPooling2D(pool_size=(2,2))(x_heads_4)\n",
        "x_heads_4_pool = Conv2D(filters=128, kernel_size=3, padding='same', activation='relu', kernel_initializer='he_uniform')(x_heads_4_pool)\n",
        "x_heads_4_pool = BatchNormalization()(x_heads_4_pool)\n",
        "\n",
        "# Decoder\n",
        "x_centroids_5 = Conv2DTranspose(filters=64, kernel_size=3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_uniform')(x_centroids_4)\n",
        "x_centroids_5 = BatchNormalization()(x_centroids_5)\n",
        "x_heads_5 = Conv2DTranspose(filters=64, kernel_size=3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_uniform')(x_heads_4_pool)\n",
        "x_heads_5 = BatchNormalization()(x_heads_5)\n",
        "\n",
        "x_centroids_6 = Conv2DTranspose(filters=32, kernel_size=3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_uniform')(x_centroids_5)\n",
        "x_centroids_6 = BatchNormalization()(x_centroids_6)\n",
        "x_heads_6 = Conv2DTranspose(filters=32, kernel_size=3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_uniform')(x_heads_5)\n",
        "x_heads_6 = BatchNormalization()(x_heads_6)\n",
        "\n",
        "x_centroids_7 = Conv2DTranspose(filters=16, kernel_size=3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_uniform')(x_centroids_6)\n",
        "x_centroids_7 = BatchNormalization()(x_centroids_7)\n",
        "x_heads_7 = Conv2DTranspose(filters=16, kernel_size=3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_uniform')(x_heads_6)\n",
        "x_heads_7 = BatchNormalization()(x_heads_7)\n",
        "\n",
        "x_centroids_8 = Conv2DTranspose(filters=8, kernel_size=3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_uniform')(x_centroids_7)\n",
        "x_centroids_8 = BatchNormalization()(x_centroids_8)\n",
        "x_heads_8 = Conv2DTranspose(filters=8, kernel_size=3, strides=(2,2), padding='same', activation='relu', kernel_initializer='he_uniform')(x_heads_7)\n",
        "x_heads_8 = BatchNormalization()(x_heads_8)\n",
        "\n",
        "\n",
        "# Output layer\n",
        "heads_output = Conv2D(filters=1, kernel_size=1, strides=1, activation='relu', kernel_initializer='he_uniform', name='heads_layer')(x_heads_8)  #pointwise convolution\n",
        "centroids_output = Concatenate(axis=-1)([x_centroids_8, x_heads_8])\n",
        "centroids_output = Conv2D(filters=1, kernel_size=1, strides=1, activation='relu', kernel_initializer='he_uniform', name='centroids_layer')(centroids_output)\n",
        "\n",
        "# Defining the model\n",
        "model = tf.keras.Model(input_layer, [heads_output, centroids_output])"
      ],
      "metadata": {
        "id": "scejdJQIawn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), \n",
        "              loss={'heads_layer': 'mse',\n",
        "                    'centroids_layer': 'mse'})"
      ],
      "metadata": {
        "id": "vEDfbRgobZHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting\n",
        "no_epochs=12\n",
        "batch_size=3 \n",
        "history=model.fit(x=train_data, \n",
        "                  y={'heads_layer': gt_data_heads,\n",
        "                     'centroids_layer': gt_data_centroids}, \n",
        "                  validation_data=(validation_data, {'heads_layer': gt_validation_heads,\n",
        "                                                     'centroids_layer': gt_validation_centroids}),\n",
        "                  batch_size=batch_size, \n",
        "                  epochs=no_epochs, \n",
        "                  shuffle=True)"
      ],
      "metadata": {
        "id": "CxToofSMbdBM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}